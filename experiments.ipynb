{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from training import train_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_directory = os.path.expanduser('~')\n",
    "model_directory = os.path.join(home_directory, 'Documents', 'MIE1620_Project', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Warning: Log dir /home/ernesto/Documents/MIE1620_Project/output/experiment53_td3_days_1/experiment53_td3_days_1/experiment53_td3_days_1_s42 already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to /home/ernesto/Documents/MIE1620_Project/output/experiment53_td3_days_1/experiment53_td3_days_1/experiment53_td3_days_1_s42/progress.txt\u001b[0m\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t actor pi: 8194, \t critic q: 16514, \n",
      "\u001b[0m\n",
      "Cost constraint:  17.32040650284075\n",
      "---------------------------------------\n",
      "|           Episode |               0 |\n",
      "|             EpRet |          -0.237 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           1e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               1 |\n",
      "|             EpRet |          -0.179 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           2e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               2 |\n",
      "|             EpRet |            1.62 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           3e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               3 |\n",
      "|             EpRet |         -0.0582 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           4e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               4 |\n",
      "|             EpRet |          -0.304 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           5e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               5 |\n",
      "|             EpRet |           -1.93 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           6e+03 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|           Episode |               6 |\n",
      "|             EpRet |           0.309 |\n",
      "|             EpLen |           1e+03 |\n",
      "|            EpCost |               0 |\n",
      "| TotalEnvInteracts |           7e+03 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m agent_params\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchkpt_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: exp_dir,\n\u001b[1;32m     24\u001b[0m })\n\u001b[1;32m     25\u001b[0m training_params \u001b[38;5;241m=\u001b[39m base_training_params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MIE1620_Project/training.py:96\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent_params, training_params, model_directory, identifier)\u001b[0m\n\u001b[1;32m     91\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Move action back to NumPy if it's a tensor\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# print(f\"Action shape: {action.shape}, dtype: {action.dtype}\")\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Step environment and receive next state, reward, done, and info (with cost)\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m obs_next, reward, cost, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m cost \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Extract cost from info\u001b[39;00m\n\u001b[1;32m     99\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/wrappers/time_limit.py:47\u001b[0m, in \u001b[0;36mSafeTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     observation, reward, cost, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/wrappers/env_checker.py:36\u001b[0m, in \u001b[0;36mSafePassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/builder.py:209\u001b[0m, in \u001b[0;36mBuilder.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m     info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost_exception\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Reward processing\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# Constraint violations\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cost())\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/builder.py:256\u001b[0m, in \u001b[0;36mBuilder._reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the current rewards.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Call exactly once per step.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Intrinsic reward for uprightness\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mreward_conf\u001b[38;5;241m.\u001b[39mreward_orientation:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/tasks/safe_navigation/goal/goal_level0.py:41\u001b[0m, in \u001b[0;36mGoalLevel0.calculate_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dist_goal \u001b[38;5;241m-\u001b[39m dist_goal) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal\u001b[38;5;241m.\u001b[39mreward_distance\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dist_goal \u001b[38;5;241m=\u001b[39m dist_goal\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_achieved\u001b[49m:\n\u001b[1;32m     42\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal\u001b[38;5;241m.\u001b[39mreward_goal\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/tasks/safe_navigation/goal/goal_level0.py:61\u001b[0m, in \u001b[0;36mGoalLevel0.goal_achieved\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Whether the goal of task is achieved.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# pylint: disable-next=no-member\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_goal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/bases/base_task.py:200\u001b[0m, in \u001b[0;36mBaseTask.dist_goal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the distance from the agent to the goal XY position.\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease make sure you have added goal into env.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/safety_gymnasium/bases/base_agent.py:459\u001b[0m, in \u001b[0;36mBaseAgent.dist_xy\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    457\u001b[0m     pos \u001b[38;5;241m=\u001b[39m pos[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    458\u001b[0m agent_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magent_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_id = 'SafetyPointGoal1-v0' # Change environment depending on the experiments\n",
    "\n",
    "base_agent_params = {\n",
    "    'tau': 0.95,\n",
    "    'gamma': 0.99,\n",
    "}\n",
    "\n",
    "base_training_params = {\n",
    "    'num_episodes': 1000, \n",
    "    'plot_interval': 100,\n",
    "    'episode_interval': 1,\n",
    "    'model_type': 'TD3Lagrangian' #Use: SAC, TD3, DDPG, SACLagrangian, DDPGLagrangian, TD3Lagrangian, CVPO\n",
    "}\n",
    "\n",
    "# Run experiment\n",
    "identifier = f\"experiment53_td3_days_1\"\n",
    "exp_dir = os.path.join(model_directory, identifier)\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "agent_params = base_agent_params.copy()\n",
    "agent_params.update({\n",
    "    'chkpt_dir': exp_dir,\n",
    "})\n",
    "training_params = base_training_params.copy()\n",
    "train_agent(env_id, agent_params, training_params, exp_dir, identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
